# CS224N assignment
---------------

 투빅스 텍스트 세미나 CS224N 과제파일 입니다. 과제 커리큘럼은 [CS224N](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html#schedule) 스케줄에서 확인할 수 있으며 아래에도 링크가 첨부되어 있습니다. 
<br/>

## Introduction
---------------
### [1. Assignment 1: Exploring Word Vectors](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a1_preview/exploring_word_vectors.html)
#### 소개
- Welcome to CS224N! We'll be using Python throughout the course. If you've got a good Python setup already, great! But make sure that it is at least Python version 3.5. If not, the easiest thing to do is to make sure you have at least 3GB free on your computer and then to head over to (https://www.anaconda.com/download/) and install the Python 3 version of Anaconda. It will work on any operating system. 

- After you have installed conda, close any open terminals you might have. Then open a new terminal and run the following command:

```
conda install gensim
```
- Homework 1 (only) is a Jupyter Notebook. With the above done you should be able to get underway by typing:

  > jupyter notebook exploring_word_vectors.ipynb


- making reduced-dimensionality co-occurrence representation
<p align="center">
<img src="https://github.com/mrsalehi/CS224n-Assignments/blob/master/figures/svd.jpg?raw=true" alt="drawing" width="400"/>
</p>

- word2vec embedding
<p align="center">
<img src="https://github.com/mrsalehi/CS224n-Assignments/blob/master/figures/analogy.jpg?raw=true" alt="drawing" width="400"/>
</p>



#### 과제
- [assignment1](https://github.com/Tobigs-team/Text-Seminar/tree/master/assignment/assignment1)에서 확인하실 수 있습니다.
<br/>





### [2. Assignment 2: word2vec](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a2.pdf)
#### 소개 
- Implement Skip-gram model 
<p align="center">
<img src="https://github.com/mrsalehi/CS224n-Assignments/blob/master/figures/word2vec.jpg?raw=true" alt="drawing" width="500"/>
</p>


#### 과제
- [assignment2](https://github.com/Tobigs-team/Text-Seminar/tree/master/assignment/assignment2)에서 확인하실 수 있습니다.
<br/>





### [3. Assignment 3: Dependency Parsing](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a3.pdf)
#### 소개
- In this assignment, you will build a neural dependency parser using PyTorch. In Part 1, you will learn
about two general neural network techniques (Adam Optimization and Dropout) that you will use to build
the dependency parser in Part 2. In Part 2, you will implement and train the dependency parser, before
analyzing a few erroneous dependency parses.

<p align="center">
<img src="https://github.com/mrsalehi/CS224n-Assignments/blob/master/figures/dependency-parsing.jpg?raw=true" alt="drawing" width="600"/>
</p>


#### 과제
- [assignment3](https://github.com/Tobigs-team/Text-Seminar/tree/master/assignment/assignment3)에서 확인하실 수 있습니다.
<br/>





### [4. Assignment 4](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a4.pdf)
#### 소개
- This assignment is split into two sections: Neural Machine Translation with RNNs and Analyzing NMT
Systems. The first is primarily coding and implementation focused, whereas the second entirely consists
of written, analysis questions. If you get stuck on the first section, you can always work on the second.
That being said, the NMT system is more complicated than the neural networks we have previously constructed within this class and takes about 4 hours to train on a GPU. Thus, we strongly recommend you
get started early with this assignment. Finally, the notation and implementation of the NMT system is a
bit tricky, so if you ever get stuck along the way, please come to Office Hours so that the TAs can support you.

<p align="center">
<img src="https://github.com/mrsalehi/CS224n-Assignments/blob/master/figures/nmt.jpg?raw=true" alt="drawing" width="450"/>
</p>


#### 과제
- [assignment4](https://github.com/Tobigs-team/Text-Seminar/tree/master/assignment/assignment4)에서 확인하실 수 있습니다.
